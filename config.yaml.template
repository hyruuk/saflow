# Saflow Configuration Template
# Copy this file to config.yaml and replace all <PLACEHOLDERS> with actual values

# =============================================================================
# Paths Configuration
# =============================================================================
paths:
  # Root data directory (where sourcedata/, derivatives/, processed/ live)
  data_root: <DATA_ROOT>  # e.g., /media/hyruuk/YH_storage/DATA/saflow

  # Data paths (relative to data_root)
  raw: sourcedata/
  derivatives: derivatives/
  features: features/

  # Project-specific paths (in saflow/ project directory)
  reports: ./reports
  logs: ./logs
  venv: ./env

  # HPC-specific paths
  slurm_output: ./logs/slurm
  tmp: /tmp  # or /scratch on HPC

  # FreeSurfer subjects directory (in data_root)
  freesurfer_subjects_dir: fs_subjects/  # Will be resolved relative to data_root

# =============================================================================
# BIDS Configuration
# =============================================================================
bids:
  task_name: gradCPT

  # Subject list (deliberately excludes 16, 25, 27 as per cc_saflow)
  subjects:
    - "04"
    - "05"
    - "06"
    - "07"
    - "08"
    - "09"
    - "10"
    - "11"
    - "12"
    - "13"
    - "14"
    - "15"
    - "17"
    - "18"
    - "19"
    - "20"
    - "21"
    - "22"
    - "23"
    - "24"
    - "26"
    - "28"
    - "29"
    - "30"
    - "31"
    - "32"
    - "33"
    - "34"
    - "35"
    - "36"
    - "37"
    - "38"

  sessions:
    - recording

  # Task runs (gradCPT)
  task_runs:
    - "02"
    - "03"
    - "04"
    - "05"
    - "06"
    - "07"

  # Resting state run (for future QC)
  rest_runs:
    - "01"
    - "08"

# =============================================================================
# Preprocessing Configuration
# =============================================================================
preprocessing:
  # Resample at the start of preprocessing (null = keep original sampling rate)
  # 600 Hz recommended: clean 2x downsampling from 1200 Hz, sufficient for analyses up to 200 Hz
  resample_sfreq: 600.0

  filter:
    lowcut: 0.5
    highcut: 200.0
    notch:
      - 60.0

  ica:
    method: fastica
    n_components: 20  # null = automatic
    random_state: 0

  autoreject:
    n_interpolate:
      - 1
      - 4
      - 32
    consensus:
      - 0.1
      - 0.2
      - 0.3
      - 0.5

# =============================================================================
# Source Reconstruction Configuration
# =============================================================================
source_reconstruction:
  method: dSPM
  snr: 3.0

  # Default atlases for ROI extraction (used by apply_atlas.py)
  # These are applied after morphing to fsaverage template
  # Short names are mapped to full MNE/FreeSurfer names internally
  # Available short names:
  #   - aparc.a2009s (Destrieux atlas, 148 ROIs)
  #   - aparc (Desikan-Killiany, 68 ROIs)
  #   - schaefer_100 (Schaefer 100 parcels, 7 networks)
  #   - schaefer_200 (Schaefer 200 parcels, 7 networks)
  #   - schaefer_400 (Schaefer 400 parcels, 7 networks)
  atlases:
    - aparc.a2009s
    - schaefer_100
    - schaefer_200
    - schaefer_400

# =============================================================================
# Behavioral Analysis Configuration
# =============================================================================
behavioral:
  vtc:
    window_size: 20  # Number of trials for rolling VTC computation

    # VTC filter parameters (applied during BIDS generation)
    filter:
      type: "gaussian"  # "gaussian" or "butterworth"
      gaussian_fwhm: 9  # Full width at half maximum (for Gaussian)
      butterworth_order: 3  # Filter order (for Butterworth)
      butterworth_cutoff: 0.05  # Normalized cutoff frequency (for Butterworth)

# =============================================================================
# Analysis Configuration (CRITICAL: Sensor vs Source)
# =============================================================================
# This section controls whether the pipeline operates in sensor-level or
# source-level analysis space.
#
# Phase 1 (current refactoring): sensor-level replication
# Phase 2 (post-refactoring): source-level extension
analysis:
  # Analysis space: "sensor", "source", or "atlas"
  # - sensor: Channel-level analysis (MEG sensors)
  # - source: Vertex-level analysis (cortical surface)
  # - atlas: ROI-level analysis (parcellated regions)
  space: sensor

  # TWO-TRACK PROCESSING: Continuous vs Epochs
  # Enables comparison between continuous (ICA-only) and epoched (ICA+AutoReject) processing
  # Both tracks can be run in parallel and compared for robustness
  processing_tracks:
    - continuous  # ICA-cleaned continuous data
    - epochs      # ICA+AutoReject epoched data

  # Source-specific settings (used when space="source")
  source:
    morph_to_template: true  # Morph to fsaverage

  # Atlas-specific settings (used when space="atlas")
  atlas:
    name: aparc.a2009s      # Parcellation: aparc.a2009s, aparc, HCPMMP1, Schaefer2018_*
    morph_to_template: true  # Morph to fsaverage before applying atlas

  # Zone bounds for IN/OUT trial classification (computed on-demand from VTC)
  # Format: [lower_percentile, upper_percentile]
  # Trials with VTC < lower_percentile → IN zone (stable performance)
  # Trials with VTC ≥ upper_percentile → OUT zone (variable performance)
  # Trials in between → MID zone (excluded from IN/OUT comparisons)
  #
  # Common options:
  #   [25, 75] - Quartile split (default, good balance)
  #   [50, 50] - Median split (maximum IN/OUT trials, no MID zone)
  #   [10, 90] - Decile split (conservative, many MID trials)
  #   [33, 67] - Tertile split
  inout_bounds: [25, 75]  # Default: focus on quartile split

  # Epoch timing relative to stimulus event marker (t=0 = 0% intensity, stimulus onset)
  # In gradCPT, stimuli gradually fade in (0%→100%) then fade out (100%→0%) over ~800ms each.
  # The event marker is placed at stimulus onset (0% intensity, start of fade-in).
  #
  # Epoch window (relative to event marker):
  #   t=0:      0% intensity (stimulus onset, start of fade-in)
  #   tmin:     50% intensity (rising phase) - epoch starts here
  #   midpoint: 100% intensity (peak visibility)
  #   tmax:     50% intensity (falling phase) - epoch ends here
  #
  # This captures the high-visibility portion of the stimulus (~852ms).
  epochs:
    tmin: 0.426   # 50% intensity (rising) - epoch start
    tmax: 1.278   # 50% intensity (falling) - epoch end

# =============================================================================
# Feature Extraction Configuration
# =============================================================================
features:
  # Frequency bands for spectral analysis
  frequency_bands:
    delta:
      - 2
      - 4
    theta:
      - 4
      - 8
    alpha:
      - 8
      - 12
    lobeta:
      - 12
      - 20
    hibeta:
      - 20
      - 30
    gamma1:
      - 30
      - 60
    gamma2:
      - 60
      - 90
    gamma3:
      - 90
      - 120

  # Welch PSD parameters (can specify multiple parameter sets)
  welch_psd:
    - name: "default"
      window_sec: 2.0       # Window length in seconds
      overlap: 0.5          # Overlap fraction (0-1)
      average: "mean"       # Averaging method: 'mean' or 'median'

  # FOOOF parameters
  fooof:
    - name: "default"
      freq_range: [2, 40]   # Frequency range for fitting
      peak_width_limits: [0.5, 12.0]
      max_n_peaks: 8
      min_peak_height: 0.0
      peak_threshold: 2.0
      aperiodic_mode: "fixed"  # 'fixed' or 'knee'

  # Complexity metrics configuration (using antropy library)
  complexity:
    # Lempel-Ziv Complexity (LZC)
    lzc:
      - name: "median"
        normalize: true
        symbolize: "median"  # Binarize using median threshold

    # Entropy measures
    entropy:
      - name: "permutation"
        metric: "perm_entropy"
        order: 3              # Embedding dimension
        delay: 1              # Time delay
        normalize: true

      - name: "spectral"
        metric: "spectral_entropy"
        sf: null              # Sampling frequency (auto-detected)
        method: "welch"
        normalize: true

      - name: "sample"
        metric: "sample_entropy"
        order: 2
        tolerance: 0.2        # Tolerance (fraction of std)

      - name: "approximate"
        metric: "app_entropy"
        order: 2
        tolerance: 0.2

      - name: "svd"
        metric: "svd_entropy"
        order: 3
        delay: 1
        normalize: true

    # Fractal dimension measures
    fractal:
      - name: "higuchi"
        metric: "higuchi_fd"
        kmax: 10              # Maximum k value

      - name: "petrosian"
        metric: "petrosian_fd"

      - name: "katz"
        metric: "katz_fd"

      - name: "dfa"
        metric: "detrended_fluctuation"

# =============================================================================
# Computing Configuration
# =============================================================================
computing:
  n_jobs: -1  # -1 = use all available cores

  slurm:
    enabled: true
    account: ""  # Leave empty if not using SLURM/HPC, or set to your SLURM account name
    partition: ""  # Leave empty to use cluster default, or specify partition name

    # Resource allocations per pipeline stage
    preprocessing:
      cpus: 12
      mem: 32G
      time: "12:00:00"

    source_reconstruction:
      cpus: 1
      mem: 64G
      time: "2:00:00"

    atlas:
      cpus: 1
      mem: 96G
      time: "0:30:00"

    features:
      cpus: 12
      mem: 32G
      time: "6:00:00"

    statistics:
      cpus: 12
      mem: 32G
      time: "24:00:00"

    classification:
      cpus: 24
      mem: 32G
      time: "168:00:00"

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  to_file: true
  to_console: true

# =============================================================================
# Statistics Configuration
# =============================================================================
statistics:
  # Default statistical test (the test statistic to compute)
  test: paired_ttest  # 'paired_ttest', 'independent_ttest'

  # Multiple comparison correction method
  # Options:
  #   - none: No correction (not recommended)
  #   - fdr: Benjamini-Hochberg FDR (controls false discovery rate)
  #   - bonferroni: Bonferroni correction (controls FWER, conservative)
  #   - tmax: Maximum statistic permutation (controls FWER, recommended)
  correction: tmax

  # FDR method ('bh' = Benjamini-Hochberg, 'by' = Benjamini-Yekutieli)
  fdr_method: bh

  # Number of permutations (for tmax correction)
  n_permutations: 10000

  # Significance threshold
  alpha: 0.05

  # Features to analyze (can be overridden via CLI)
  features:
    - fooof_exponent
    - psd_alpha
    - psd_theta

  # Effect sizes to compute
  effect_sizes:
    - cohens_d
    - hedges_g

  # Enable visualization
  visualize: true

# =============================================================================
# Reproducibility Configuration
# =============================================================================
reproducibility:
  random_seed: 42
  track_git_hash: true
  save_provenance: true
