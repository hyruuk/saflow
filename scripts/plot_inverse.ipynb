{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Compute inverse solution\n",
    "\n",
    "The inverse solution pipeline performs source reconstruction starting either\n",
    "from raw/epoched data (*.fif* format) specified by the user or from the output\n",
    "of the Preprocessing pipeline (cleaned raw data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Annalisa Pascarella <a.pascarella@iac.cnr.it>\n",
    "# License: BSD (3-clause)\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import nipype.pipeline.engine as pe\n",
    "import nipype.interfaces.io as nio\n",
    "\n",
    "import ephypype\n",
    "from ephypype.nodes import create_iterator\n",
    "from ephypype.datasets import fetch_omega_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fetch the data first. It is around 675 MB download.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = op.join(op.dirname(ephypype.__file__), 'examples')\n",
    "data_path = fetch_omega_dataset(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then read the parameters for experiment and inverse problem from a\n",
    ":download:`json <https://github.com/neuropycon/ephypype/blob/master/examples/params.json>`\n",
    "file and print it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment parameters': {'NJOBS': 1,\n",
      "                           'data_type': 'fif',\n",
      "                           'session_ids': ['ses-recording'],\n",
      "                           'subject_ids': ['sub-07'],\n",
      "                           'subjects_dir': 'saflow_anat'}}\n",
      "{'inverse parameters': {'img_method': 'MNE',\n",
      "                        'method': 'LCMV',\n",
      "                        'noise_cov_fname': '*noise*.ds',\n",
      "                        'parcellation': 'aparc',\n",
      "                        'snr': 1.0,\n",
      "                        'spacing': 'oct-6'}}\n"
     ]
    }
   ],
   "source": [
    "import json  # noqa\n",
    "import pprint  # noqa\n",
    "params = json.load(open(\"params.json\"))\n",
    "\n",
    "pprint.pprint({'experiment parameters': params[\"general\"]})\n",
    "subject_ids = params[\"general\"][\"subject_ids\"]  # sub-003\n",
    "session_ids = params[\"general\"][\"session_ids\"]  # ses-0001\n",
    "NJOBS = params[\"general\"][\"NJOBS\"]\n",
    "\n",
    "pprint.pprint({'inverse parameters': params[\"inverse\"]})\n",
    "spacing = params[\"inverse\"]['spacing']  # ico-5 vs oct-6\n",
    "snr = params[\"inverse\"]['snr']  # use smaller SNR for raw data\n",
    "inv_method = params[\"inverse\"]['img_method']  # sLORETA, MNE, dSPM, LCMV\n",
    "parc = params[\"inverse\"]['parcellation']  # parcellation to use: 'aparc' vs 'aparc.a2009s'  # noqa\n",
    "# noise covariance matrix filename template\n",
    "noise_cov_fname = params[\"inverse\"]['noise_cov_fname']\n",
    "\n",
    "# set sbj dir path, i.e. where the FS folfers are\n",
    "subjects_dir = op.join(data_path, params[\"general\"][\"subjects_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create our workflow and specify the `base_dir` which tells\n",
    "nipype the directory in which to store the outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow directory within the `base_dir`\n",
    "src_reconstruction_pipeline_name = 'source_reconstruction_' + \\\n",
    "    inv_method + '_' + parc.replace('.', '')\n",
    "\n",
    "main_workflow = pe.Workflow(name=src_reconstruction_pipeline_name)\n",
    "main_workflow.base_dir = data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a node to pass input filenames to DataGrabber from nipype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "infosource = create_iterator(['subject_id', 'session_id'],\n",
    "                             [subject_ids, session_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a node to grab data. The template_args in this node iterate upon\n",
    "the values in the infosource node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource = pe.Node(interface=nio.DataGrabber(infields=['subject_id'],\n",
    "                                               outfields=['raw_file', 'trans_file']),  # noqa\n",
    "                     name='datasource')\n",
    "\n",
    "datasource.inputs.base_directory = data_path\n",
    "datasource.inputs.template = '*%s/%s/meg/%s*gradCPT*%s.fif'\n",
    "\n",
    "datasource.inputs.template_args = dict(\n",
    "        raw_file=[['subject_id', 'session_id', 'subject_id', 'epo']],\n",
    "        trans_file=[['subject_id', 'session_id', 'subject_id', \"epotrans\"]])\n",
    "\n",
    "datasource.inputs.sort_filelist = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ephypype creates for us a pipeline which can be connected to these\n",
    "nodes we created. The inverse solution pipeline is implemented by the\n",
    "function\n",
    ":func:`ephypype.pipelines.preproc_meeg.create_pipeline_source_reconstruction`\n",
    "thus to instantiate the inverse pipeline node, we import it and pass our\n",
    "parameters to it.\n",
    "The inverse pipeline contains three nodes that wrap the MNE Python functions\n",
    "that perform the source reconstruction steps.\n",
    "\n",
    "In particular, the three nodes are:\n",
    "\n",
    "* :class:`ephypype.interfaces.mne.LF_computation.LFComputation` compute the\n",
    "  Lead Field matrix\n",
    "* :class:`ephypype.interfaces.mne.Inverse_solution.NoiseCovariance` computes\n",
    "  the noise covariance matrix\n",
    "* :class:`ephypype.interfaces.mne.Inverse_solution.InverseSolution` estimates\n",
    "  the time series of the neural sources on a set of dipoles grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** {} *noise*.ds\n"
     ]
    }
   ],
   "source": [
    "from ephypype.pipelines import create_pipeline_source_reconstruction  # noqa\n",
    "inv_sol_workflow = create_pipeline_source_reconstruction(\n",
    "    data_path, subjects_dir, spacing=spacing, inv_method=inv_method, parc=parc,\n",
    "    noise_cov_fname=noise_cov_fname, is_epoched=True, events_id = {'Freq': 21, 'Rare': 31}, t_min=0, t_max=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then connect the nodes two at a time. First, we connect the two outputs\n",
    "(subject_id and session_id) of the infosource node to the datasource node.\n",
    "So, these two nodes taken together can grab data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_workflow.connect(infosource, 'subject_id', datasource, 'subject_id')\n",
    "main_workflow.connect(infosource, 'session_id', datasource, 'session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the inputnode of the preproc_workflow. Things will become\n",
    "clearer in a moment when we plot the graph of the workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_workflow.connect(infosource, 'subject_id',\n",
    "                      inv_sol_workflow, 'inputnode.sbj_id')\n",
    "main_workflow.connect(datasource, 'raw_file',\n",
    "                      inv_sol_workflow, 'inputnode.raw')\n",
    "main_workflow.connect(datasource, 'trans_file',\n",
    "                      inv_sol_workflow, 'inputnode.trans_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first write the workflow graph (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_workflow.write_graph(graph2use='colored')  # colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualize it. Take a moment to pause and notice how the connections\n",
    "here correspond to how we connected the nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # noqa\n",
    "img = plt.imread(op.join(data_path, src_reconstruction_pipeline_name, 'graph.png'))  # noqa\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are now ready to execute our workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200616-09:06:58,432 nipype.workflow INFO:\n",
      "\t Workflow source_reconstruction_MNE_aparc settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "200616-09:06:58,437 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "200616-09:06:58,438 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 1/1.\n",
      "200616-09:06:58,484 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"source_reconstruction_MNE_aparc.datasource\" in \"/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/source_reconstruction_MNE_aparc/_session_id_ses-recording_subject_id_sub-07/datasource\".\n",
      "200616-09:06:58,491 nipype.workflow INFO:\n",
      "\t [Node] Running \"datasource\" (\"nipype.interfaces.io.DataGrabber\")\n",
      "200616-09:06:58,496 nipype.workflow INFO:\n",
      "\t [Node] Finished \"source_reconstruction_MNE_aparc.datasource\".\n",
      "200616-09:07:00,440 nipype.workflow INFO:\n",
      "\t [Job 0] Completed (source_reconstruction_MNE_aparc.datasource).\n",
      "200616-09:07:00,442 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 3 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 1/1.\n",
      "200616-09:07:00,490 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"source_reconstruction_MNE_aparc.inv_sol_pipeline.create_noise_cov\" in \"/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/source_reconstruction_MNE_aparc/inv_sol_pipeline/_session_id_ses-recording_subject_id_sub-07/create_noise_cov\".\n",
      "200616-09:07:00,494 nipype.workflow INFO:\n",
      "\t [Node] Running \"create_noise_cov\" (\"ephypype.interfaces.mne.Inverse_solution.NoiseCovariance\")\n",
      "*** /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_noise.ds \n",
      "\n",
      "ds directory : /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_noise.ds\n",
      "200616-09:07:00,499 nipype.workflow WARNING:\n",
      "\t Storing result file without outputs\n",
      "200616-09:07:00,501 nipype.workflow WARNING:\n",
      "\t [Node] Error on \"source_reconstruction_MNE_aparc.inv_sol_pipeline.create_noise_cov\" (/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/source_reconstruction_MNE_aparc/inv_sol_pipeline/_session_id_ses-recording_subject_id_sub-07/create_noise_cov)\n",
      "200616-09:07:02,442 nipype.workflow ERROR:\n",
      "\t Node create_noise_cov.a0 failed to run on host hyruuk-home.\n",
      "200616-09:07:02,443 nipype.workflow ERROR:\n",
      "\t Saving crash info to /home/hyruuk/GitHub/saflow/scripts/crash-20200616-090702-hyruuk-create_noise_cov.a0-bd772c75-1963-42b7-af77-f320caf1b9e3.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/plugins/multiproc.py\", line 67, in run_node\n",
      "    result[\"result\"] = node.run(updatehash=updatehash)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 516, in run\n",
      "    result = self._run_interface(execute=True)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 635, in _run_interface\n",
      "    return self._run_command(execute)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 741, in _run_command\n",
      "    result = self._interface.run(cwd=outdir)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/interfaces/base/core.py\", line 397, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/interfaces/mne/Inverse_solution.py\", line 288, in _run_interface\n",
      "    op.join(data_path, cov_fname_in), raw_filename)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/compute_inv_problem.py\", line 48, in compute_noise_cov\n",
      "    er_raw, cov_fname = _get_er_data(fname_template)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/compute_inv_problem.py\", line 94, in _get_er_data\n",
      "    er_raw = read_raw_ctf(er_fname)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/ctf/ctf.py\", line 62, in read_raw_ctf\n",
      "    clean_names=clean_names, verbose=verbose)\n",
      "  File \"<decorator-gen-162>\", line 21, in __init__\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/ctf/ctf.py\", line 100, in __init__\n",
      "    res4 = _read_res4(directory)  # Read the magical res4 file\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/ctf/res4.py\", line 102, in _read_res4\n",
      "    name = _make_ctf_name(dsdir, 'res4')\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/ctf/res4.py\", line 21, in _make_ctf_name\n",
      "    raise IOError('Standard file %s not found' % fname)\n",
      "OSError: Standard file /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_noise.ds/sub-07_ses-recording_noise.res4 not found\n",
      "\n",
      "200616-09:07:02,445 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 1/1.\n",
      "200616-09:07:02,486 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"source_reconstruction_MNE_aparc.inv_sol_pipeline.define_epochs\" in \"/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/source_reconstruction_MNE_aparc/inv_sol_pipeline/_session_id_ses-recording_subject_id_sub-07/define_epochs\".\n",
      "200616-09:07:02,489 nipype.workflow INFO:\n",
      "\t [Node] Running \"define_epochs\" (\"ephypype.interfaces.mne.preproc.DefineEpochs\")\n",
      "Opening raw data file /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_task-gradCPT_run-02_meg_epo.fif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/preproc.py:445: RuntimeWarning: This filename (/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_task-gradCPT_run-02_meg_epo.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz or _meg.fif\n",
      "  raw = read_raw_fif(fif_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200616-09:07:02,512 nipype.workflow WARNING:\n",
      "\t Storing result file without outputs\n",
      "200616-09:07:02,514 nipype.workflow WARNING:\n",
      "\t [Node] Error on \"source_reconstruction_MNE_aparc.inv_sol_pipeline.define_epochs\" (/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/source_reconstruction_MNE_aparc/inv_sol_pipeline/_session_id_ses-recording_subject_id_sub-07/define_epochs)\n",
      "200616-09:07:04,444 nipype.workflow ERROR:\n",
      "\t Node define_epochs.a0 failed to run on host hyruuk-home.\n",
      "200616-09:07:04,445 nipype.workflow ERROR:\n",
      "\t Saving crash info to /home/hyruuk/GitHub/saflow/scripts/crash-20200616-090704-hyruuk-define_epochs.a0-0b82d38a-6984-43ce-a6c6-6890165bed14.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/plugins/multiproc.py\", line 67, in run_node\n",
      "    result[\"result\"] = node.run(updatehash=updatehash)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 516, in run\n",
      "    result = self._run_interface(execute=True)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 635, in _run_interface\n",
      "    return self._run_command(execute)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 741, in _run_command\n",
      "    result = self._interface.run(cwd=outdir)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/interfaces/base/core.py\", line 397, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/interfaces/mne/preproc.py\", line 272, in _run_interface\n",
      "    events_file, decim)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/preproc.py\", line 445, in _define_epochs\n",
      "    raw = read_raw_fif(fif_file)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/fiff/raw.py\", line 453, in read_raw_fif\n",
      "    preload=preload, verbose=verbose)\n",
      "  File \"<decorator-gen-169>\", line 21, in __init__\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/fiff/raw.py\", line 80, in __init__\n",
      "    preload, do_check_fname)\n",
      "  File \"<decorator-gen-170>\", line 21, in _read_raw_file\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/fiff/raw.py\", line 161, in _read_raw_file\n",
      "    raise ValueError('No raw data in %s' % fname_rep)\n",
      "ValueError: No raw data in /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_task-gradCPT_run-02_meg_epo.fif\n",
      "\n",
      "200616-09:07:04,447 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 1/1.\n",
      "200616-09:07:04,491 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"source_reconstruction_MNE_aparc.inv_sol_pipeline.LF_computation\" in \"/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/source_reconstruction_MNE_aparc/inv_sol_pipeline/_session_id_ses-recording_subject_id_sub-07/LF_computation\".\n",
      "200616-09:07:04,493 nipype.workflow INFO:\n",
      "\t [Node] Running \"LF_computation\" (\"ephypype.interfaces.mne.LF_computation.LFComputation\")\n",
      "\n",
      " *** fwd_filename /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_task-gradCPT_run-02_meg_epo-oct-6-fwd.fif ***\n",
      "\n",
      "\n",
      "*** Computing FWD matrix /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_task-gradCPT_run-02_meg_epo-oct-6-fwd.fif ***\n",
      "\n",
      "Embedding : jquery.js\n",
      "Embedding : jquery-ui.min.js\n",
      "Embedding : bootstrap.min.js\n",
      "Embedding : jquery-ui.min.css\n",
      "Embedding : bootstrap.min.css\n",
      "\n",
      "*** BEM solution file /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/saflow_anat/sub-07/bem/sub-07-5120-bem-sol.fif exists!!! ***\n",
      "\n",
      "*** subject dir /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/saflow_anat\n",
      "\n",
      "*** source space file /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/saflow_anat/sub-07/bem/sub-07-oct-6-src.fif exists!!!\n",
      "\n",
      "    Reading a source space...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "src space contains 2 spaces and 8196 vertices\n",
      "Source space          : <SourceSpaces: [<surface (lh), n_vertices=146781, n_used=4098>, <surface (rh), n_vertices=146730, n_used=4098>] MRI (surface RAS) coords, subject 'sub-07'>\n",
      "MRI -> head transform : /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/sub-07/ses-recording/meg/sub-07_ses-recording_task-gradCPT_run-02_meg_epotrans.fif\n",
      "Measurement data      : sub-07_ses-recording_task-gradCPT_run-02_meg_epo.fif\n",
      "Conductor model   : /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/saflow_anat/sub-07/bem/sub-07-5120-bem-sol.fif\n",
      "Accurate field computations\n",
      "Do computations in head coordinates\n",
      "Free source orientations\n",
      "\n",
      "Read 2 source spaces a total of 8196 active source locations\n",
      "\n",
      "Coordinate transformation: MRI (surface RAS) -> head\n",
      "     0.999271 -0.014442 -0.035330      -2.46 mm\n",
      "     0.022598  0.969840  0.242693       7.69 mm\n",
      "     0.030760 -0.243314  0.969460      30.83 mm\n",
      "     0.000000  0.000000  0.000000       1.00\n",
      "\n",
      "Read 270 MEG channels from info\n",
      "99 coil definitions read\n",
      "Coordinate transformation: MEG device -> head\n",
      "     0.999725 -0.009151 -0.021573      -2.14 mm\n",
      "     0.006410  0.992280 -0.123848      -6.33 mm\n",
      "     0.022540  0.123676  0.992067      55.14 mm\n",
      "     0.000000  0.000000  0.000000       1.00\n",
      "MEG coil definitions created in head coordinates.\n",
      "Source spaces are now in head coordinates.\n",
      "\n",
      "Setting up the BEM model using /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/saflow_anat/sub-07/bem/sub-07-5120-bem-sol.fif...\n",
      "\n",
      "Loading surfaces...\n",
      "Homogeneous model surface loaded.\n",
      "\n",
      "Loading the solution matrix...\n",
      "\n",
      "Loaded linear_collocation BEM solution from /home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/saflow_anat/sub-07/bem/sub-07-5120-bem-sol.fif\n",
      "Employing the head->MRI coordinate transform with the BEM model.\n",
      "BEM model sub-07-5120-bem-sol.fif is now set up\n",
      "\n",
      "Source spaces are in head coordinates.\n",
      "Checking that the sources are inside the surface and at least    5.0 mm away (will take a few...)\n",
      "    Skipping interior check for 1391 sources that fit inside a sphere of radius   49.8 mm\n",
      "    Skipping solid angle check for 0 points using Qhull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Skipping interior check for 1376 sources that fit inside a sphere of radius   49.8 mm\n",
      "    Skipping solid angle check for 0 points using Qhull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200616-09:07:06,446 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 28.03/28.23, Free processors: 0/1.\n",
      "                     Currently running:\n",
      "                       * source_reconstruction_MNE_aparc.inv_sol_pipeline.LF_computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1 source space point omitted because of the    5.0-mm distance limit.\n",
      "\n",
      "Setting up compensation data...\n",
      "    270 out of 270 channels have the compensation set.\n",
      "200616-09:07:07,317 nipype.workflow WARNING:\n",
      "\t Storing result file without outputs\n",
      "200616-09:07:07,318 nipype.workflow WARNING:\n",
      "\t [Node] Error on \"source_reconstruction_MNE_aparc.inv_sol_pipeline.LF_computation\" (/home/hyruuk/Downloads/storage/Yann/saflow_DATA/neuropycon-help/bids/source_reconstruction_MNE_aparc/inv_sol_pipeline/_session_id_ses-recording_subject_id_sub-07/LF_computation)\n",
      "200616-09:07:08,448 nipype.workflow ERROR:\n",
      "\t Node LF_computation.a0 failed to run on host hyruuk-home.\n",
      "200616-09:07:08,449 nipype.workflow ERROR:\n",
      "\t Saving crash info to /home/hyruuk/GitHub/saflow/scripts/crash-20200616-090708-hyruuk-LF_computation.a0-74951b2b-8327-405a-986f-4b10948c0681.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/plugins/multiproc.py\", line 67, in run_node\n",
      "    result[\"result\"] = node.run(updatehash=updatehash)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 516, in run\n",
      "    result = self._run_interface(execute=True)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 635, in _run_interface\n",
      "    return self._run_command(execute)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/nodes.py\", line 741, in _run_command\n",
      "    result = self._interface.run(cwd=outdir)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/interfaces/base/core.py\", line 397, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/interfaces/mne/LF_computation.py\", line 106, in _run_interface\n",
      "    self.fwd_filename)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype-0.2-py3.7.egg/ephypype/compute_fwd_problem.py\", line 189, in _compute_fwd_sol\n",
      "    n_jobs=2)\n",
      "  File \"<decorator-gen-272>\", line 21, in make_forward_solution\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/forward/_make_forward.py\", line 604, in make_forward_solution\n",
      "    infos, coil_types, n_jobs)\n",
      "  File \"<decorator-gen-266>\", line 21, in _compute_forwards\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/forward/_compute_forward.py\", line 918, in _compute_forwards\n",
      "    _prep_field_computation(rr, bem, fwd_data, n_jobs)\n",
      "  File \"<decorator-gen-265>\", line 21, in _prep_field_computation\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/forward/_compute_forward.py\", line 770, in _prep_field_computation\n",
      "    compensator = _make_ctf_comp_coils(info, coils)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/forward/_compute_forward.py\", line 284, in _make_ctf_comp_coils\n",
      "    compensator = make_compensator(info, 0, comp_num, True)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/compensator.py\", line 99, in make_compensator\n",
      "    C2 = _make_compensator(info, to)\n",
      "  File \"/home/hyruuk/anaconda3/envs/neuropycon/lib/python3.7/site-packages/mne/io/compensator.py\", line 63, in _make_compensator\n",
      "    ' found' % grade)\n",
      "ValueError: Desired compensation matrix (grade = 3) not found\n",
      "\n",
      "200616-09:07:08,452 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 0 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 1/1.\n",
      "200616-09:07:10,449 nipype.workflow INFO:\n",
      "\t ***********************************\n",
      "200616-09:07:10,449 nipype.workflow ERROR:\n",
      "\t could not run node: source_reconstruction_MNE_aparc.inv_sol_pipeline.create_noise_cov.a0\n",
      "200616-09:07:10,450 nipype.workflow INFO:\n",
      "\t crashfile: /home/hyruuk/GitHub/saflow/scripts/crash-20200616-090702-hyruuk-create_noise_cov.a0-bd772c75-1963-42b7-af77-f320caf1b9e3.pklz\n",
      "200616-09:07:10,450 nipype.workflow ERROR:\n",
      "\t could not run node: source_reconstruction_MNE_aparc.inv_sol_pipeline.define_epochs.a0\n",
      "200616-09:07:10,451 nipype.workflow INFO:\n",
      "\t crashfile: /home/hyruuk/GitHub/saflow/scripts/crash-20200616-090704-hyruuk-define_epochs.a0-0b82d38a-6984-43ce-a6c6-6890165bed14.pklz\n",
      "200616-09:07:10,451 nipype.workflow ERROR:\n",
      "\t could not run node: source_reconstruction_MNE_aparc.inv_sol_pipeline.LF_computation.a0\n",
      "200616-09:07:10,452 nipype.workflow INFO:\n",
      "\t crashfile: /home/hyruuk/GitHub/saflow/scripts/crash-20200616-090708-hyruuk-LF_computation.a0-74951b2b-8327-405a-986f-4b10948c0681.pklz\n",
      "200616-09:07:10,452 nipype.workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-badc3d685f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Run workflow locally on 1 CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmain_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MultiProc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_procs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNJOBS\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"execution\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"create_report\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%dT%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"execution\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"write_provenance\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/plugins/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_node_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# close any open resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuropycon/lib/python3.7/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"Workflow did not execute cleanly. \"\u001b[0m \u001b[0;34m\"Check log for details\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         )\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "main_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\n",
    "\n",
    "# Run workflow locally on 1 CPU\n",
    "main_workflow.run(plugin='MultiProc', plugin_args={'n_procs': NJOBS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the source reconstruction matrix stored in the workflow\n",
    "directory defined by `base_dir`. This matrix can be used as input of\n",
    "the Connectivity pipeline.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>To use this pipeline, we need a cortical segmentation of MRI\n",
    "  data, that could be provided by Freesurfer</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # noqa\n",
    "from ephypype.gather import get_results  # noqa\n",
    "from visbrain.objects import BrainObj, ColorbarObj, SceneObj  # noqa\n",
    "\n",
    "time_series_files, label_files = get_results(main_workflow.base_dir,\n",
    "                                             main_workflow.name,\n",
    "                                             pipeline='inverse')\n",
    "\n",
    "time_pts = 30\n",
    "\n",
    "sc = SceneObj(size=(800, 500), bgcolor=(0, 0, 0))\n",
    "lh_file = op.join(subjects_dir, 'fsaverage', 'label/lh.aparc.annot')\n",
    "rh_file = op.join(subjects_dir, 'fsaverage', 'label/rh.aparc.annot')\n",
    "cmap = 'bwr'\n",
    "txtcolor = 'white'\n",
    "for inverse_file, label_file in zip(time_series_files, label_files):\n",
    "    # Load files :\n",
    "    with open(label_file, 'rb') as f:\n",
    "        ar = pickle.load(f)\n",
    "        names, xyz, colors = ar['ROI_names'], ar['ROI_coords'], ar['ROI_colors']  # noqa\n",
    "    ts = np.squeeze(np.load(inverse_file))\n",
    "    cen = np.array([k.mean(0) for k in xyz])\n",
    "\n",
    "    # Get the data of the left / right hemisphere :\n",
    "    lh_data, rh_data = ts[::2, time_pts], ts[1::2, time_pts]\n",
    "    clim = (ts[:, time_pts].min(), ts[:, time_pts].max())\n",
    "    roi_names = [k[0:-3] for k in np.array(names)[::2]]\n",
    "\n",
    "    # Left hemisphere outside :\n",
    "    b_obj_li = BrainObj('white', translucent=False, hemisphere='left')\n",
    "    b_obj_li.parcellize(lh_file, select=roi_names, data=lh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_li, rotate='left')\n",
    "\n",
    "    # Left hemisphere inside :\n",
    "    b_obj_lo = BrainObj('white',  translucent=False, hemisphere='left')\n",
    "    b_obj_lo.parcellize(lh_file, select=roi_names, data=lh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_lo, col=1, rotate='right')\n",
    "\n",
    "    # Right hemisphere outside :\n",
    "    b_obj_ro = BrainObj('white',  translucent=False, hemisphere='right')\n",
    "    b_obj_ro.parcellize(rh_file, select=roi_names, data=rh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_ro, row=1, rotate='right')\n",
    "\n",
    "    # Right hemisphere inside :\n",
    "    b_obj_ri = BrainObj('white',  translucent=False, hemisphere='right')\n",
    "    b_obj_ri.parcellize(rh_file, select=roi_names, data=rh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_ri, row=1, col=1, rotate='left')\n",
    "\n",
    "    # Add the colorbar :\n",
    "    cbar = ColorbarObj(b_obj_li, txtsz=15, cbtxtsz=20, txtcolor=txtcolor,\n",
    "                       cblabel='Intensity')\n",
    "    sc.add_to_subplot(cbar, col=2, row_span=2)\n",
    "\n",
    "sc.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neuropycon] *",
   "language": "python",
   "name": "conda-env-neuropycon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
