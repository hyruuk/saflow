{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Compute inverse solution\n",
    "\n",
    "The inverse solution pipeline performs source reconstruction starting either\n",
    "from raw/epoched data (*.fif* format) specified by the user or from the output\n",
    "of the Preprocessing pipeline (cleaned raw data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200609-15:23:57,76 nipype.utils INFO:\n",
      "\t Running nipype version 1.2.3 (latest: 1.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Authors: Annalisa Pascarella <a.pascarella@iac.cnr.it>\n",
    "# License: BSD (3-clause)\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import nipype.pipeline.engine as pe\n",
    "import nipype.interfaces.io as nio\n",
    "\n",
    "import ephypype\n",
    "from ephypype.nodes import create_iterator\n",
    "from ephypype.datasets import fetch_omega_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fetch the data first. It is around 675 MB download.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ucb8d3f3261e2a2ff28ad9d7c5a5.dl.dropboxusercontent.com/zip_download_get/AcvfBJaBulcH856XfEK8X_jkqeJo7TDJNtn5TiEsiAl0OHHJ5wtQOSGNlRZqoHyUVR2FGzwBhZVQU5PY5J0KC1UyKuNBnjM0_pj_XS8r9bg2qw?dl=1 (799.4 MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9247b0eea6c40369db2872ffcc890e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=838209637.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File saved as /home/karim/anaconda3/envs/neuropycon/lib/python3.7/site-packages/ephypype/examples/sample_BIDS_omega.zip.\n",
      "\n",
      "Extracting files. This may take a while ...\n"
     ]
    }
   ],
   "source": [
    "base_path = op.join(op.dirname(ephypype.__file__), 'examples')\n",
    "data_path = fetch_omega_dataset(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then read the parameters for experiment and inverse problem from a\n",
    ":download:`json <https://github.com/neuropycon/ephypype/blob/master/examples/params.json>`\n",
    "file and print it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'params.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-004641da0158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'experiment parameters'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"general\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'params.json'"
     ]
    }
   ],
   "source": [
    "import json  # noqa\n",
    "import pprint  # noqa\n",
    "params = json.load(open(\"params.json\"))\n",
    "\n",
    "pprint.pprint({'experiment parameters': params[\"general\"]})\n",
    "subject_ids = params[\"general\"][\"subject_ids\"]  # sub-003\n",
    "session_ids = params[\"general\"][\"session_ids\"]  # ses-0001\n",
    "NJOBS = params[\"general\"][\"NJOBS\"]\n",
    "\n",
    "pprint.pprint({'inverse parameters': params[\"inverse\"]})\n",
    "spacing = params[\"inverse\"]['spacing']  # ico-5 vs oct-6\n",
    "snr = params[\"inverse\"]['snr']  # use smaller SNR for raw data\n",
    "inv_method = params[\"inverse\"]['img_method']  # sLORETA, MNE, dSPM, LCMV\n",
    "parc = params[\"inverse\"]['parcellation']  # parcellation to use: 'aparc' vs 'aparc.a2009s'  # noqa\n",
    "# noise covariance matrix filename template\n",
    "noise_cov_fname = params[\"inverse\"]['noise_cov_fname']\n",
    "\n",
    "# set sbj dir path, i.e. where the FS folfers are\n",
    "subjects_dir = op.join(data_path, params[\"general\"][\"subjects_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create our workflow and specify the `base_dir` which tells\n",
    "nipype the directory in which to store the outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow directory within the `base_dir`\n",
    "src_reconstruction_pipeline_name = 'source_reconstruction_' + \\\n",
    "    inv_method + '_' + parc.replace('.', '')\n",
    "\n",
    "main_workflow = pe.Workflow(name=src_reconstruction_pipeline_name)\n",
    "main_workflow.base_dir = data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a node to pass input filenames to DataGrabber from nipype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infosource = create_iterator(['subject_id', 'session_id'],\n",
    "                             [subject_ids, session_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a node to grab data. The template_args in this node iterate upon\n",
    "the values in the infosource node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource = pe.Node(interface=nio.DataGrabber(infields=['subject_id'],\n",
    "                                               outfields=['raw_file', 'trans_file']),  # noqa\n",
    "                     name='datasource')\n",
    "\n",
    "datasource.inputs.base_directory = data_path\n",
    "datasource.inputs.template = '*%s/%s/meg/%s*rest*%s.fif'\n",
    "\n",
    "datasource.inputs.template_args = dict(\n",
    "        raw_file=[['subject_id', 'session_id', 'subject_id', '0_60*ica']],\n",
    "        trans_file=[['subject_id', 'session_id', 'subject_id', \"-trans\"]])\n",
    "\n",
    "datasource.inputs.sort_filelist = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ephypype creates for us a pipeline which can be connected to these\n",
    "nodes we created. The inverse solution pipeline is implemented by the\n",
    "function\n",
    ":func:`ephypype.pipelines.preproc_meeg.create_pipeline_source_reconstruction`\n",
    "thus to instantiate the inverse pipeline node, we import it and pass our\n",
    "parameters to it.\n",
    "The inverse pipeline contains three nodes that wrap the MNE Python functions\n",
    "that perform the source reconstruction steps.\n",
    "\n",
    "In particular, the three nodes are:\n",
    "\n",
    "* :class:`ephypype.interfaces.mne.LF_computation.LFComputation` compute the\n",
    "  Lead Field matrix\n",
    "* :class:`ephypype.interfaces.mne.Inverse_solution.NoiseCovariance` computes\n",
    "  the noise covariance matrix\n",
    "* :class:`ephypype.interfaces.mne.Inverse_solution.InverseSolution` estimates\n",
    "  the time series of the neural sources on a set of dipoles grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ephypype.pipelines import create_pipeline_source_reconstruction  # noqa\n",
    "inv_sol_workflow = create_pipeline_source_reconstruction(\n",
    "    data_path, subjects_dir, spacing=spacing, inv_method=inv_method, parc=parc,\n",
    "    noise_cov_fname=noise_cov_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then connect the nodes two at a time. First, we connect the two outputs\n",
    "(subject_id and session_id) of the infosource node to the datasource node.\n",
    "So, these two nodes taken together can grab data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_workflow.connect(infosource, 'subject_id', datasource, 'subject_id')\n",
    "main_workflow.connect(infosource, 'session_id', datasource, 'session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the inputnode of the preproc_workflow. Things will become\n",
    "clearer in a moment when we plot the graph of the workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_workflow.connect(infosource, 'subject_id',\n",
    "                      inv_sol_workflow, 'inputnode.sbj_id')\n",
    "main_workflow.connect(datasource, 'raw_file',\n",
    "                      inv_sol_workflow, 'inputnode.raw')\n",
    "main_workflow.connect(datasource, 'trans_file',\n",
    "                      inv_sol_workflow, 'inputnode.trans_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first write the workflow graph (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_workflow.write_graph(graph2use='colored')  # colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualize it. Take a moment to pause and notice how the connections\n",
    "here correspond to how we connected the nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # noqa\n",
    "img = plt.imread(op.join(data_path, src_reconstruction_pipeline_name, 'graph.png'))  # noqa\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are now ready to execute our workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\n",
    "\n",
    "# Run workflow locally on 1 CPU\n",
    "main_workflow.run(plugin='MultiProc', plugin_args={'n_procs': NJOBS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the source reconstruction matrix stored in the workflow\n",
    "directory defined by `base_dir`. This matrix can be used as input of\n",
    "the Connectivity pipeline.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>To use this pipeline, we need a cortical segmentation of MRI\n",
    "  data, that could be provided by Freesurfer</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # noqa\n",
    "from ephypype.gather import get_results  # noqa\n",
    "from visbrain.objects import BrainObj, ColorbarObj, SceneObj  # noqa\n",
    "\n",
    "time_series_files, label_files = get_results(main_workflow.base_dir,\n",
    "                                             main_workflow.name,\n",
    "                                             pipeline='inverse')\n",
    "\n",
    "time_pts = 30\n",
    "\n",
    "sc = SceneObj(size=(800, 500), bgcolor=(0, 0, 0))\n",
    "lh_file = op.join(subjects_dir, 'fsaverage', 'label/lh.aparc.annot')\n",
    "rh_file = op.join(subjects_dir, 'fsaverage', 'label/rh.aparc.annot')\n",
    "cmap = 'bwr'\n",
    "txtcolor = 'white'\n",
    "for inverse_file, label_file in zip(time_series_files, label_files):\n",
    "    # Load files :\n",
    "    with open(label_file, 'rb') as f:\n",
    "        ar = pickle.load(f)\n",
    "        names, xyz, colors = ar['ROI_names'], ar['ROI_coords'], ar['ROI_colors']  # noqa\n",
    "    ts = np.squeeze(np.load(inverse_file))\n",
    "    cen = np.array([k.mean(0) for k in xyz])\n",
    "\n",
    "    # Get the data of the left / right hemisphere :\n",
    "    lh_data, rh_data = ts[::2, time_pts], ts[1::2, time_pts]\n",
    "    clim = (ts[:, time_pts].min(), ts[:, time_pts].max())\n",
    "    roi_names = [k[0:-3] for k in np.array(names)[::2]]\n",
    "\n",
    "    # Left hemisphere outside :\n",
    "    b_obj_li = BrainObj('white', translucent=False, hemisphere='left')\n",
    "    b_obj_li.parcellize(lh_file, select=roi_names, data=lh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_li, rotate='left')\n",
    "\n",
    "    # Left hemisphere inside :\n",
    "    b_obj_lo = BrainObj('white',  translucent=False, hemisphere='left')\n",
    "    b_obj_lo.parcellize(lh_file, select=roi_names, data=lh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_lo, col=1, rotate='right')\n",
    "\n",
    "    # Right hemisphere outside :\n",
    "    b_obj_ro = BrainObj('white',  translucent=False, hemisphere='right')\n",
    "    b_obj_ro.parcellize(rh_file, select=roi_names, data=rh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_ro, row=1, rotate='right')\n",
    "\n",
    "    # Right hemisphere inside :\n",
    "    b_obj_ri = BrainObj('white',  translucent=False, hemisphere='right')\n",
    "    b_obj_ri.parcellize(rh_file, select=roi_names, data=rh_data, cmap=cmap)\n",
    "    sc.add_to_subplot(b_obj_ri, row=1, col=1, rotate='left')\n",
    "\n",
    "    # Add the colorbar :\n",
    "    cbar = ColorbarObj(b_obj_li, txtsz=15, cbtxtsz=20, txtcolor=txtcolor,\n",
    "                       cblabel='Intensity')\n",
    "    sc.add_to_subplot(cbar, col=2, row_span=2)\n",
    "\n",
    "sc.preview()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
